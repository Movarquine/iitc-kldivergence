By MH Gencer


Go to 'code.py' above for the Python script (requires Python 3.9 or higher). See below for a short explanation. 


# Integrated Information Theory of Consciousness (IITC)


IITC is a relatively new theory of consciousness marshaled by Giulio Tononi (e.g., 2008, 2014, 2016). The main goal of IITC is to establish the foundations for a distinctively mathematical explanation of phenomenal experience. To this end, the theory offers a framework in which consciousness, as we experience it, results from highly integrated informational processes that can be specified using a set of well-defined mathematical concepts. The hope is to open new avenues for both empirical and purely mathematical research on individually necessary and jointly sufficient conditions for consciousness.


# Kullback-Leibler Divergence (KLD)
KLD is one of the statistical concepts used in IITC. Specifically, KLD is used in IITC to conceptualize the difference between what the conscious entity generates as output (i.e., conscious experience) and what its brain (or any other physical substrate of consciousness) takes as input (e.g., sense-data), both construed as probability distributions. Formally, KLD quantifies how much one probability distribution differs from another probability distribution in terms of 'nats', bits of information.


In the interactive graph generated by 'code.py', KLD is stated using the standard notation KL(P || Q).


The distance dKL is the Kullback-Leibler divergence, and measures how many bits of information are lost when q is used to approximate p, in the sense that if you developed an optimal data compression algorithm to compress data drawn from a probability distribution q, it would on average require dKL(p, q) more bits to compress data drawn from a probability distribution p than if the algorithm had been optimized for p [23]. This has been argued to be the be the best measure because of its desirable properties related to information geometry [24, 25].


# References


